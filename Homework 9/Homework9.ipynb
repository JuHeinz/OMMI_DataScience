{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#create dataframes\n",
    "train  = pd.read_csv(\"aug_train.csv\")\n",
    "test  = pd.read_csv(\"aug_test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task1 Data clean, imputation\n",
    "#1. in experience, replace >20 to 21; <1 to 1, and convert this as a numerical column\n",
    "\n",
    "train.loc[train[\"experience\"] == \">20\", \"experience\"] = 21\n",
    "train.loc[train[\"experience\"] == \"<1\", \"experience\"] = 1\n",
    "test.loc[test[\"experience\"] == \">20\", \"experience\"] = 21\n",
    "test.loc[test[\"experience\"] == \"<1\", \"experience\"] = 1\n",
    "\n",
    "#convert to numeric\n",
    "test[\"experience\"] = pd.to_numeric(test[\"experience\"])\n",
    "train[\"experience\"] = pd.to_numeric(train[\"experience\"])\n",
    "\n",
    "\n",
    "#2. in last_new_job, replace >4 to 5; never to 0, and convert this as a numerical column\n",
    "\n",
    "train.loc[train[\"last_new_job\"] == \">4\", \"last_new_job\"] = 5\n",
    "train.loc[train[\"last_new_job\"] == \"never\", \"last_new_job\"] = 0\n",
    "test.loc[test[\"last_new_job\"] == \">4\", \"last_new_job\"] = 5\n",
    "test.loc[test[\"last_new_job\"] == \"never\", \"last_new_job\"] = 0\n",
    "\n",
    "#convert to numeric\n",
    "test[\"last_new_job\"] = pd.to_numeric(test[\"last_new_job\"])\n",
    "train[\"last_new_job\"] = pd.to_numeric(train[\"last_new_job\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. If the column is categorical, impute the missing value as its mode. \n",
    "#If the column is numerical, impute the missing value as its median\n",
    "\n",
    "for column in test:\n",
    "    if test[column].dtype == \"object\":       \n",
    "        test[column] = test[column].fillna(test[column].mode()[0])\n",
    "    else:\n",
    "        test[column] = test[column].fillna(test[column].median())\n",
    "\n",
    "for column in train:\n",
    "    if train[column].dtype == \"object\":       \n",
    "        train[column] = train[column].fillna(train[column].mode()[0])\n",
    "    else:\n",
    "        train[column] = train[column].fillna(train[column].median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Further preparations for classification:\n",
    "#get X and y\n",
    "features= [\"city_development_index\",\"gender\",\"relevent_experience\",\"enrolled_university\",\"education_level\",\"major_discipline\",\"experience\",\"company_type\",\"last_new_job\",\"training_hours\"]\n",
    "y_train = train[[\"target\"]]\n",
    "X_train = train[features]\n",
    "y_test = test[[\"target\"]]\n",
    "X_test = test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Further preparations for classification:\n",
    "#Transform categorical data with one Hot encoding\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "categorical_features =['gender','relevent_experience', 'enrolled_university', 'education_level','major_discipline','company_type']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('encoder', OneHotEncoder(), categorical_features)],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task2 Classification\n",
    "#1. Build a classification model from the training set (you can use any algorithm)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Use decision tree classifier, adding class_weight=balanced to hopefully account for imbalanced data set\n",
    "clf = DecisionTreeClassifier(class_weight='balanced')\n",
    "#fit model\n",
    "clf.fit(X_train, y_train)\n",
    "#make predicion (on training)\n",
    "y_train_pred = clf.predict(X_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. generate the confusion matrix for the training set and calculate the accuracy, precision, recall, and F1-score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "cm_train = confusion_matrix(y_train, y_train_pred)\n",
    "accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "precision_train = precision_score(y_train, y_train_pred)\n",
    "recall_train = recall_score(y_train, y_train_pred)\n",
    "f1_train = f1_score(y_train, y_train_pred)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Applying the model in the test set and generating the prediction\n",
    "y_test_pred = clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. generate the confusion matrix from the test set and calculate the accuracy, precision, recall, and F1-score\n",
    "cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "precision_test = precision_score(y_test, y_test_pred)\n",
    "recall_test = recall_score(y_test, y_test_pred)\n",
    "f1_test = f1_score(y_test, y_test_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SET | TEST SET\n",
      "Confusion matrix\n",
      "[[1563    2]\n",
      " [   0  535]]\n",
      "[[60 18]\n",
      " [12 10]]\n",
      "Accuracy: 0.9990476190476191 | 0.7\n",
      "Precision: 0.9962756052141527 | 0.35714285714285715\n",
      "Recall: 1.0 | 0.45454545454545453\n",
      "F1 Score: 0.9981343283582089 | 0.4\n"
     ]
    }
   ],
   "source": [
    "#5. compare the results between the training and test set\n",
    "print(\"TRAINING SET | TEST SET\")\n",
    "print(\"Confusion matrix\")\n",
    "print(cm_train)\n",
    "print(cm_test)\n",
    "print(\"Accuracy:\", accuracy_train,\"|\", accuracy_test)\n",
    "print(\"Precision:\", precision_train,\"|\", precision_test)\n",
    "print(\"Recall:\", recall_train,\"|\", recall_test)\n",
    "print(\"F1 Score:\", f1_train, \"|\", f1_test)\n",
    "\n",
    "#All the scores for the training data are much higher. This is no surprise, since the model is trained on this data after all and therefore the classifier is very tuned into the data of the training set. This is called overfitting.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
